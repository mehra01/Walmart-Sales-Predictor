{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "dataset_url = 'https://www.kaggle.com/competitions/walmart-recruiting-store-sales-forecasting/data'\n",
    "od.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features.csv', 'features.csv.zip', 'sampleSubmission.csv', 'sampleSubmission.csv.zip', 'stores.csv', 'submission.csv', 'test.csv', 'test.csv.zip', 'train.csv', 'train.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "DATA_DIR = './data/walmart-recruiting-store-sales-forecasting'\n",
    "print(os.listdir(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(DATA_DIR+'/features.csv.zip') as zpf:\n",
    "    zpf.extractall(path=DATA_DIR)\n",
    "with ZipFile(DATA_DIR+'/sampleSubmission.csv.zip') as zpf:\n",
    "    zpf.extractall(path=DATA_DIR)\n",
    "with ZipFile(DATA_DIR+'/test.csv.zip') as zpf:\n",
    "    zpf.extractall(path=DATA_DIR)\n",
    "with ZipFile(DATA_DIR+'/train.csv.zip') as zpf:\n",
    "    zpf.extractall(path=DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(DATA_DIR+'/train.csv')\n",
    "test_df = pd.read_csv(DATA_DIR+'/test.csv')\n",
    "submission_df = pd.read_csv(DATA_DIR+'/sampleSubmission.csv')\n",
    "features_df = pd.read_csv(DATA_DIR+'/features.csv')\n",
    "stores_df = pd.read_csv(DATA_DIR+'/stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421565</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>508.37</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421566</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>628.10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421567</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>1061.02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421568</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>760.01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421569</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>1076.80</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421570 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  Weekly_Sales  IsHoliday\n",
       "0           1     1  2010-02-05      24924.50      False\n",
       "1           1     1  2010-02-12      46039.49       True\n",
       "2           1     1  2010-02-19      41595.55      False\n",
       "3           1     1  2010-02-26      19403.54      False\n",
       "4           1     1  2010-03-05      21827.90      False\n",
       "...       ...   ...         ...           ...        ...\n",
       "421565     45    98  2012-09-28        508.37      False\n",
       "421566     45    98  2012-10-05        628.10      False\n",
       "421567     45    98  2012-10-12       1061.02      False\n",
       "421568     45    98  2012-10-19        760.01      False\n",
       "421569     45    98  2012-10-26       1076.80      False\n",
       "\n",
       "[421570 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Date</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8185</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>76.05</td>\n",
       "      <td>3.639</td>\n",
       "      <td>4842.29</td>\n",
       "      <td>975.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2449.97</td>\n",
       "      <td>3169.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>77.50</td>\n",
       "      <td>3.614</td>\n",
       "      <td>9090.48</td>\n",
       "      <td>2268.58</td>\n",
       "      <td>582.74</td>\n",
       "      <td>5797.47</td>\n",
       "      <td>1514.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>79.37</td>\n",
       "      <td>3.614</td>\n",
       "      <td>3789.94</td>\n",
       "      <td>1827.31</td>\n",
       "      <td>85.72</td>\n",
       "      <td>744.84</td>\n",
       "      <td>2150.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>82.84</td>\n",
       "      <td>3.737</td>\n",
       "      <td>2961.49</td>\n",
       "      <td>1047.07</td>\n",
       "      <td>204.19</td>\n",
       "      <td>363.00</td>\n",
       "      <td>1059.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>45</td>\n",
       "      <td>2013-07-26</td>\n",
       "      <td>76.06</td>\n",
       "      <td>3.804</td>\n",
       "      <td>212.02</td>\n",
       "      <td>851.73</td>\n",
       "      <td>2.06</td>\n",
       "      <td>10.88</td>\n",
       "      <td>1864.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8190 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
       "0         1  2010-02-05        42.31       2.572        NaN        NaN   \n",
       "1         1  2010-02-12        38.51       2.548        NaN        NaN   \n",
       "2         1  2010-02-19        39.93       2.514        NaN        NaN   \n",
       "3         1  2010-02-26        46.63       2.561        NaN        NaN   \n",
       "4         1  2010-03-05        46.50       2.625        NaN        NaN   \n",
       "...     ...         ...          ...         ...        ...        ...   \n",
       "8185     45  2013-06-28        76.05       3.639    4842.29     975.03   \n",
       "8186     45  2013-07-05        77.50       3.614    9090.48    2268.58   \n",
       "8187     45  2013-07-12        79.37       3.614    3789.94    1827.31   \n",
       "8188     45  2013-07-19        82.84       3.737    2961.49    1047.07   \n",
       "8189     45  2013-07-26        76.06       3.804     212.02     851.73   \n",
       "\n",
       "      MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
       "0           NaN        NaN        NaN  211.096358         8.106      False  \n",
       "1           NaN        NaN        NaN  211.242170         8.106       True  \n",
       "2           NaN        NaN        NaN  211.289143         8.106      False  \n",
       "3           NaN        NaN        NaN  211.319643         8.106      False  \n",
       "4           NaN        NaN        NaN  211.350143         8.106      False  \n",
       "...         ...        ...        ...         ...           ...        ...  \n",
       "8185       3.00    2449.97    3169.69         NaN           NaN      False  \n",
       "8186     582.74    5797.47    1514.93         NaN           NaN      False  \n",
       "8187      85.72     744.84    2150.36         NaN           NaN      False  \n",
       "8188     204.19     363.00    1059.46         NaN           NaN      False  \n",
       "8189       2.06      10.88    1864.57         NaN           NaN      False  \n",
       "\n",
       "[8190 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>202307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>37392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>205863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>34875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "      <td>202505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B</td>\n",
       "      <td>70713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>A</td>\n",
       "      <td>155078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B</td>\n",
       "      <td>125833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>126512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>A</td>\n",
       "      <td>207499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>B</td>\n",
       "      <td>112238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>A</td>\n",
       "      <td>219622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>A</td>\n",
       "      <td>200898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>B</td>\n",
       "      <td>123737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>B</td>\n",
       "      <td>57197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>B</td>\n",
       "      <td>93188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>B</td>\n",
       "      <td>120653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>A</td>\n",
       "      <td>203819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>A</td>\n",
       "      <td>203742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>B</td>\n",
       "      <td>140167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>B</td>\n",
       "      <td>119557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>B</td>\n",
       "      <td>114533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>A</td>\n",
       "      <td>203819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>B</td>\n",
       "      <td>128107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>A</td>\n",
       "      <td>152513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>A</td>\n",
       "      <td>204184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>A</td>\n",
       "      <td>206302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>B</td>\n",
       "      <td>93638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>C</td>\n",
       "      <td>42988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>203750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>A</td>\n",
       "      <td>203007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>A</td>\n",
       "      <td>39690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>A</td>\n",
       "      <td>158114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>B</td>\n",
       "      <td>103681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>A</td>\n",
       "      <td>39910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>C</td>\n",
       "      <td>39910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>C</td>\n",
       "      <td>39690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>A</td>\n",
       "      <td>184109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>A</td>\n",
       "      <td>155083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>A</td>\n",
       "      <td>196321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>C</td>\n",
       "      <td>39690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>C</td>\n",
       "      <td>41062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>C</td>\n",
       "      <td>39910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Store Type    Size\n",
       "0       1    A  151315\n",
       "1       2    A  202307\n",
       "2       3    B   37392\n",
       "3       4    A  205863\n",
       "4       5    B   34875\n",
       "5       6    A  202505\n",
       "6       7    B   70713\n",
       "7       8    A  155078\n",
       "8       9    B  125833\n",
       "9      10    B  126512\n",
       "10     11    A  207499\n",
       "11     12    B  112238\n",
       "12     13    A  219622\n",
       "13     14    A  200898\n",
       "14     15    B  123737\n",
       "15     16    B   57197\n",
       "16     17    B   93188\n",
       "17     18    B  120653\n",
       "18     19    A  203819\n",
       "19     20    A  203742\n",
       "20     21    B  140167\n",
       "21     22    B  119557\n",
       "22     23    B  114533\n",
       "23     24    A  203819\n",
       "24     25    B  128107\n",
       "25     26    A  152513\n",
       "26     27    A  204184\n",
       "27     28    A  206302\n",
       "28     29    B   93638\n",
       "29     30    C   42988\n",
       "30     31    A  203750\n",
       "31     32    A  203007\n",
       "32     33    A   39690\n",
       "33     34    A  158114\n",
       "34     35    B  103681\n",
       "35     36    A   39910\n",
       "36     37    C   39910\n",
       "37     38    C   39690\n",
       "38     39    A  184109\n",
       "39     40    A  155083\n",
       "40     41    A  196321\n",
       "41     42    C   39690\n",
       "42     43    C   41062\n",
       "43     44    C   39910\n",
       "44     45    B  118221"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115059</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115060</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115061</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115062</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115063</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115064 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  IsHoliday\n",
       "0           1     1  2012-11-02      False\n",
       "1           1     1  2012-11-09      False\n",
       "2           1     1  2012-11-16      False\n",
       "3           1     1  2012-11-23       True\n",
       "4           1     1  2012-11-30      False\n",
       "...       ...   ...         ...        ...\n",
       "115059     45    98  2013-06-28      False\n",
       "115060     45    98  2013-07-05      False\n",
       "115061     45    98  2013-07-12      False\n",
       "115062     45    98  2013-07-19      False\n",
       "115063     45    98  2013-07-26      False\n",
       "\n",
       "[115064 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1_2012-11-02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1_2012-11-09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1_2012-11-16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1_2012-11-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1_2012-11-30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115059</th>\n",
       "      <td>45_98_2013-06-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115060</th>\n",
       "      <td>45_98_2013-07-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115061</th>\n",
       "      <td>45_98_2013-07-12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115062</th>\n",
       "      <td>45_98_2013-07-19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115063</th>\n",
       "      <td>45_98_2013-07-26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115064 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Id  Weekly_Sales\n",
       "0         1_1_2012-11-02             0\n",
       "1         1_1_2012-11-09             0\n",
       "2         1_1_2012-11-16             0\n",
       "3         1_1_2012-11-23             0\n",
       "4         1_1_2012-11-30             0\n",
       "...                  ...           ...\n",
       "115059  45_98_2013-06-28             0\n",
       "115060  45_98_2013-07-05             0\n",
       "115061  45_98_2013-07-12             0\n",
       "115062  45_98_2013-07-19             0\n",
       "115063  45_98_2013-07-26             0\n",
       "\n",
       "[115064 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = raw_df.merge(stores_df, how='left', on='Store')\n",
    "merged_test_df = test_df.merge(stores_df, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421565</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>508.37</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421566</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>628.10</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421567</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>1061.02</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421568</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>760.01</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421569</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>1076.80</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421570 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  Weekly_Sales  IsHoliday Type    Size\n",
       "0           1     1  2010-02-05      24924.50      False    A  151315\n",
       "1           1     1  2010-02-12      46039.49       True    A  151315\n",
       "2           1     1  2010-02-19      41595.55      False    A  151315\n",
       "3           1     1  2010-02-26      19403.54      False    A  151315\n",
       "4           1     1  2010-03-05      21827.90      False    A  151315\n",
       "...       ...   ...         ...           ...        ...  ...     ...\n",
       "421565     45    98  2012-09-28        508.37      False    B  118221\n",
       "421566     45    98  2012-10-05        628.10      False    B  118221\n",
       "421567     45    98  2012-10-12       1061.02      False    B  118221\n",
       "421568     45    98  2012-10-19        760.01      False    B  118221\n",
       "421569     45    98  2012-10-26       1076.80      False    B  118221\n",
       "\n",
       "[421570 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.merge(features_df, how='left', on=['Date','Store','IsHoliday'])\n",
    "merged_test_df = merged_test_df.merge(features_df, how='left', on=['Date','Store','IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-05</td>\n",
       "      <td>24924.50</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>42.31</td>\n",
       "      <td>2.572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.096358</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>46039.49</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>38.51</td>\n",
       "      <td>2.548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.242170</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-19</td>\n",
       "      <td>41595.55</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>39.93</td>\n",
       "      <td>2.514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.289143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-02-26</td>\n",
       "      <td>19403.54</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.63</td>\n",
       "      <td>2.561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.319643</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-03-05</td>\n",
       "      <td>21827.90</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>46.50</td>\n",
       "      <td>2.625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.350143</td>\n",
       "      <td>8.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421565</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-09-28</td>\n",
       "      <td>508.37</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>64.88</td>\n",
       "      <td>3.997</td>\n",
       "      <td>4556.61</td>\n",
       "      <td>20.64</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1601.01</td>\n",
       "      <td>3288.25</td>\n",
       "      <td>192.013558</td>\n",
       "      <td>8.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421566</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-05</td>\n",
       "      <td>628.10</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>64.89</td>\n",
       "      <td>3.985</td>\n",
       "      <td>5046.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.82</td>\n",
       "      <td>2253.43</td>\n",
       "      <td>2340.01</td>\n",
       "      <td>192.170412</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421567</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-12</td>\n",
       "      <td>1061.02</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>54.47</td>\n",
       "      <td>4.000</td>\n",
       "      <td>1956.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.89</td>\n",
       "      <td>599.32</td>\n",
       "      <td>3990.54</td>\n",
       "      <td>192.327265</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421568</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-19</td>\n",
       "      <td>760.01</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>56.47</td>\n",
       "      <td>3.969</td>\n",
       "      <td>2004.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.18</td>\n",
       "      <td>437.73</td>\n",
       "      <td>1537.49</td>\n",
       "      <td>192.330854</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421569</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2012-10-26</td>\n",
       "      <td>1076.80</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>58.85</td>\n",
       "      <td>3.882</td>\n",
       "      <td>4018.91</td>\n",
       "      <td>58.08</td>\n",
       "      <td>100.00</td>\n",
       "      <td>211.94</td>\n",
       "      <td>858.33</td>\n",
       "      <td>192.308899</td>\n",
       "      <td>8.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>421570 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  Weekly_Sales  IsHoliday Type    Size  \\\n",
       "0           1     1  2010-02-05      24924.50      False    A  151315   \n",
       "1           1     1  2010-02-12      46039.49       True    A  151315   \n",
       "2           1     1  2010-02-19      41595.55      False    A  151315   \n",
       "3           1     1  2010-02-26      19403.54      False    A  151315   \n",
       "4           1     1  2010-03-05      21827.90      False    A  151315   \n",
       "...       ...   ...         ...           ...        ...  ...     ...   \n",
       "421565     45    98  2012-09-28        508.37      False    B  118221   \n",
       "421566     45    98  2012-10-05        628.10      False    B  118221   \n",
       "421567     45    98  2012-10-12       1061.02      False    B  118221   \n",
       "421568     45    98  2012-10-19        760.01      False    B  118221   \n",
       "421569     45    98  2012-10-26       1076.80      False    B  118221   \n",
       "\n",
       "        Temperature  Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  \\\n",
       "0             42.31       2.572        NaN        NaN        NaN        NaN   \n",
       "1             38.51       2.548        NaN        NaN        NaN        NaN   \n",
       "2             39.93       2.514        NaN        NaN        NaN        NaN   \n",
       "3             46.63       2.561        NaN        NaN        NaN        NaN   \n",
       "4             46.50       2.625        NaN        NaN        NaN        NaN   \n",
       "...             ...         ...        ...        ...        ...        ...   \n",
       "421565        64.88       3.997    4556.61      20.64       1.50    1601.01   \n",
       "421566        64.89       3.985    5046.74        NaN      18.82    2253.43   \n",
       "421567        54.47       4.000    1956.28        NaN       7.89     599.32   \n",
       "421568        56.47       3.969    2004.02        NaN       3.18     437.73   \n",
       "421569        58.85       3.882    4018.91      58.08     100.00     211.94   \n",
       "\n",
       "        MarkDown5         CPI  Unemployment  \n",
       "0             NaN  211.096358         8.106  \n",
       "1             NaN  211.242170         8.106  \n",
       "2             NaN  211.289143         8.106  \n",
       "3             NaN  211.319643         8.106  \n",
       "4             NaN  211.350143         8.106  \n",
       "...           ...         ...           ...  \n",
       "421565    3288.25  192.013558         8.684  \n",
       "421566    2340.01  192.170412         8.667  \n",
       "421567    3990.54  192.327265         8.667  \n",
       "421568    1537.49  192.330854         8.667  \n",
       "421569     858.33  192.308899         8.667  \n",
       "\n",
       "[421570 rows x 16 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Date</th>\n",
       "      <th>IsHoliday</th>\n",
       "      <th>Type</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-02</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>55.32</td>\n",
       "      <td>3.386</td>\n",
       "      <td>6766.44</td>\n",
       "      <td>5147.70</td>\n",
       "      <td>50.82</td>\n",
       "      <td>3639.90</td>\n",
       "      <td>2737.42</td>\n",
       "      <td>223.462779</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-09</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>61.24</td>\n",
       "      <td>3.314</td>\n",
       "      <td>11421.32</td>\n",
       "      <td>3370.89</td>\n",
       "      <td>40.28</td>\n",
       "      <td>4646.79</td>\n",
       "      <td>6154.16</td>\n",
       "      <td>223.481307</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-16</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>52.92</td>\n",
       "      <td>3.252</td>\n",
       "      <td>9696.28</td>\n",
       "      <td>292.10</td>\n",
       "      <td>103.78</td>\n",
       "      <td>1133.15</td>\n",
       "      <td>6612.69</td>\n",
       "      <td>223.512911</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-23</td>\n",
       "      <td>True</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>56.23</td>\n",
       "      <td>3.211</td>\n",
       "      <td>883.59</td>\n",
       "      <td>4.17</td>\n",
       "      <td>74910.32</td>\n",
       "      <td>209.91</td>\n",
       "      <td>303.32</td>\n",
       "      <td>223.561947</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-30</td>\n",
       "      <td>False</td>\n",
       "      <td>A</td>\n",
       "      <td>151315</td>\n",
       "      <td>52.34</td>\n",
       "      <td>3.207</td>\n",
       "      <td>2460.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3838.35</td>\n",
       "      <td>150.57</td>\n",
       "      <td>6966.34</td>\n",
       "      <td>223.610984</td>\n",
       "      <td>6.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115059</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-06-28</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>76.05</td>\n",
       "      <td>3.639</td>\n",
       "      <td>4842.29</td>\n",
       "      <td>975.03</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2449.97</td>\n",
       "      <td>3169.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115060</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-05</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>77.50</td>\n",
       "      <td>3.614</td>\n",
       "      <td>9090.48</td>\n",
       "      <td>2268.58</td>\n",
       "      <td>582.74</td>\n",
       "      <td>5797.47</td>\n",
       "      <td>1514.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115061</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-12</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>79.37</td>\n",
       "      <td>3.614</td>\n",
       "      <td>3789.94</td>\n",
       "      <td>1827.31</td>\n",
       "      <td>85.72</td>\n",
       "      <td>744.84</td>\n",
       "      <td>2150.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115062</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-19</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>82.84</td>\n",
       "      <td>3.737</td>\n",
       "      <td>2961.49</td>\n",
       "      <td>1047.07</td>\n",
       "      <td>204.19</td>\n",
       "      <td>363.00</td>\n",
       "      <td>1059.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115063</th>\n",
       "      <td>45</td>\n",
       "      <td>98</td>\n",
       "      <td>2013-07-26</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>118221</td>\n",
       "      <td>76.06</td>\n",
       "      <td>3.804</td>\n",
       "      <td>212.02</td>\n",
       "      <td>851.73</td>\n",
       "      <td>2.06</td>\n",
       "      <td>10.88</td>\n",
       "      <td>1864.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115064 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  Dept        Date  IsHoliday Type    Size  Temperature  \\\n",
       "0           1     1  2012-11-02      False    A  151315        55.32   \n",
       "1           1     1  2012-11-09      False    A  151315        61.24   \n",
       "2           1     1  2012-11-16      False    A  151315        52.92   \n",
       "3           1     1  2012-11-23       True    A  151315        56.23   \n",
       "4           1     1  2012-11-30      False    A  151315        52.34   \n",
       "...       ...   ...         ...        ...  ...     ...          ...   \n",
       "115059     45    98  2013-06-28      False    B  118221        76.05   \n",
       "115060     45    98  2013-07-05      False    B  118221        77.50   \n",
       "115061     45    98  2013-07-12      False    B  118221        79.37   \n",
       "115062     45    98  2013-07-19      False    B  118221        82.84   \n",
       "115063     45    98  2013-07-26      False    B  118221        76.06   \n",
       "\n",
       "        Fuel_Price  MarkDown1  MarkDown2  MarkDown3  MarkDown4  MarkDown5  \\\n",
       "0            3.386    6766.44    5147.70      50.82    3639.90    2737.42   \n",
       "1            3.314   11421.32    3370.89      40.28    4646.79    6154.16   \n",
       "2            3.252    9696.28     292.10     103.78    1133.15    6612.69   \n",
       "3            3.211     883.59       4.17   74910.32     209.91     303.32   \n",
       "4            3.207    2460.03        NaN    3838.35     150.57    6966.34   \n",
       "...            ...        ...        ...        ...        ...        ...   \n",
       "115059       3.639    4842.29     975.03       3.00    2449.97    3169.69   \n",
       "115060       3.614    9090.48    2268.58     582.74    5797.47    1514.93   \n",
       "115061       3.614    3789.94    1827.31      85.72     744.84    2150.36   \n",
       "115062       3.737    2961.49    1047.07     204.19     363.00    1059.46   \n",
       "115063       3.804     212.02     851.73       2.06      10.88    1864.57   \n",
       "\n",
       "               CPI  Unemployment  \n",
       "0       223.462779         6.573  \n",
       "1       223.481307         6.573  \n",
       "2       223.512911         6.573  \n",
       "3       223.561947         6.573  \n",
       "4       223.610984         6.573  \n",
       "...            ...           ...  \n",
       "115059         NaN           NaN  \n",
       "115060         NaN           NaN  \n",
       "115061         NaN           NaN  \n",
       "115062         NaN           NaN  \n",
       "115063         NaN           NaN  \n",
       "\n",
       "[115064 rows x 15 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 421570 entries, 0 to 421569\n",
      "Data columns (total 16 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Store         421570 non-null  int64  \n",
      " 1   Dept          421570 non-null  int64  \n",
      " 2   Date          421570 non-null  object \n",
      " 3   Weekly_Sales  421570 non-null  float64\n",
      " 4   IsHoliday     421570 non-null  bool   \n",
      " 5   Type          421570 non-null  object \n",
      " 6   Size          421570 non-null  int64  \n",
      " 7   Temperature   421570 non-null  float64\n",
      " 8   Fuel_Price    421570 non-null  float64\n",
      " 9   MarkDown1     150681 non-null  float64\n",
      " 10  MarkDown2     111248 non-null  float64\n",
      " 11  MarkDown3     137091 non-null  float64\n",
      " 12  MarkDown4     134967 non-null  float64\n",
      " 13  MarkDown5     151432 non-null  float64\n",
      " 14  CPI           421570 non-null  float64\n",
      " 15  Unemployment  421570 non-null  float64\n",
      "dtypes: bool(1), float64(10), int64(3), object(2)\n",
      "memory usage: 51.9+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Year'] = df.Date.dt.year\n",
    "    df['Month'] = df.Date.dt.month\n",
    "    df['Day'] = df.Date.dt.day\n",
    "    df['WeekOfYear'] = df.Date.dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date(merged_df)\n",
    "split_date(merged_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 421570 entries, 0 to 421569\n",
      "Data columns (total 20 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   Store         421570 non-null  int64         \n",
      " 1   Dept          421570 non-null  int64         \n",
      " 2   Date          421570 non-null  datetime64[ns]\n",
      " 3   Weekly_Sales  421570 non-null  float64       \n",
      " 4   IsHoliday     421570 non-null  bool          \n",
      " 5   Type          421570 non-null  object        \n",
      " 6   Size          421570 non-null  int64         \n",
      " 7   Temperature   421570 non-null  float64       \n",
      " 8   Fuel_Price    421570 non-null  float64       \n",
      " 9   MarkDown1     150681 non-null  float64       \n",
      " 10  MarkDown2     111248 non-null  float64       \n",
      " 11  MarkDown3     137091 non-null  float64       \n",
      " 12  MarkDown4     134967 non-null  float64       \n",
      " 13  MarkDown5     151432 non-null  float64       \n",
      " 14  CPI           421570 non-null  float64       \n",
      " 15  Unemployment  421570 non-null  float64       \n",
      " 16  Year          421570 non-null  int64         \n",
      " 17  Month         421570 non-null  int64         \n",
      " 18  Day           421570 non-null  int64         \n",
      " 19  WeekOfYear    421570 non-null  UInt32        \n",
      "dtypes: UInt32(1), bool(1), datetime64[ns](1), float64(10), int64(6), object(1)\n",
      "memory usage: 63.5+ MB\n"
     ]
    }
   ],
   "source": [
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>Dept</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "      <th>Size</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Fuel_Price</th>\n",
       "      <th>MarkDown1</th>\n",
       "      <th>MarkDown2</th>\n",
       "      <th>MarkDown3</th>\n",
       "      <th>MarkDown4</th>\n",
       "      <th>MarkDown5</th>\n",
       "      <th>CPI</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>WeekOfYear</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>150681.000000</td>\n",
       "      <td>111248.000000</td>\n",
       "      <td>137091.000000</td>\n",
       "      <td>134967.000000</td>\n",
       "      <td>151432.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.000000</td>\n",
       "      <td>421570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>22.200546</td>\n",
       "      <td>44.260317</td>\n",
       "      <td>15981.258123</td>\n",
       "      <td>136727.915739</td>\n",
       "      <td>60.090059</td>\n",
       "      <td>3.361027</td>\n",
       "      <td>7246.420196</td>\n",
       "      <td>3334.628621</td>\n",
       "      <td>1439.421384</td>\n",
       "      <td>3383.168256</td>\n",
       "      <td>4628.975079</td>\n",
       "      <td>171.201947</td>\n",
       "      <td>7.960289</td>\n",
       "      <td>2010.968591</td>\n",
       "      <td>6.449510</td>\n",
       "      <td>15.673131</td>\n",
       "      <td>25.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.785297</td>\n",
       "      <td>30.492054</td>\n",
       "      <td>22711.183519</td>\n",
       "      <td>60980.583328</td>\n",
       "      <td>18.447931</td>\n",
       "      <td>0.458515</td>\n",
       "      <td>8291.221345</td>\n",
       "      <td>9475.357325</td>\n",
       "      <td>9623.078290</td>\n",
       "      <td>6292.384031</td>\n",
       "      <td>5962.887455</td>\n",
       "      <td>39.159276</td>\n",
       "      <td>1.863296</td>\n",
       "      <td>0.796876</td>\n",
       "      <td>3.243217</td>\n",
       "      <td>8.753549</td>\n",
       "      <td>14.151887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-4988.940000</td>\n",
       "      <td>34875.000000</td>\n",
       "      <td>-2.060000</td>\n",
       "      <td>2.472000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>-265.760000</td>\n",
       "      <td>-29.100000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>135.160000</td>\n",
       "      <td>126.064000</td>\n",
       "      <td>3.879000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2079.650000</td>\n",
       "      <td>93638.000000</td>\n",
       "      <td>46.680000</td>\n",
       "      <td>2.933000</td>\n",
       "      <td>2240.270000</td>\n",
       "      <td>41.600000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>504.220000</td>\n",
       "      <td>1878.440000</td>\n",
       "      <td>132.022667</td>\n",
       "      <td>6.891000</td>\n",
       "      <td>2010.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>7612.030000</td>\n",
       "      <td>140167.000000</td>\n",
       "      <td>62.090000</td>\n",
       "      <td>3.452000</td>\n",
       "      <td>5347.450000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>1481.310000</td>\n",
       "      <td>3359.450000</td>\n",
       "      <td>182.318780</td>\n",
       "      <td>7.866000</td>\n",
       "      <td>2011.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>20205.852500</td>\n",
       "      <td>202505.000000</td>\n",
       "      <td>74.280000</td>\n",
       "      <td>3.738000</td>\n",
       "      <td>9210.900000</td>\n",
       "      <td>1926.940000</td>\n",
       "      <td>103.990000</td>\n",
       "      <td>3595.040000</td>\n",
       "      <td>5563.800000</td>\n",
       "      <td>212.416993</td>\n",
       "      <td>8.572000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>45.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>693099.360000</td>\n",
       "      <td>219622.000000</td>\n",
       "      <td>100.140000</td>\n",
       "      <td>4.468000</td>\n",
       "      <td>88646.760000</td>\n",
       "      <td>104519.540000</td>\n",
       "      <td>141630.610000</td>\n",
       "      <td>67474.850000</td>\n",
       "      <td>108519.280000</td>\n",
       "      <td>227.232807</td>\n",
       "      <td>14.313000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Store           Dept   Weekly_Sales           Size  \\\n",
       "count  421570.000000  421570.000000  421570.000000  421570.000000   \n",
       "mean       22.200546      44.260317   15981.258123  136727.915739   \n",
       "std        12.785297      30.492054   22711.183519   60980.583328   \n",
       "min         1.000000       1.000000   -4988.940000   34875.000000   \n",
       "25%        11.000000      18.000000    2079.650000   93638.000000   \n",
       "50%        22.000000      37.000000    7612.030000  140167.000000   \n",
       "75%        33.000000      74.000000   20205.852500  202505.000000   \n",
       "max        45.000000      99.000000  693099.360000  219622.000000   \n",
       "\n",
       "         Temperature     Fuel_Price      MarkDown1      MarkDown2  \\\n",
       "count  421570.000000  421570.000000  150681.000000  111248.000000   \n",
       "mean       60.090059       3.361027    7246.420196    3334.628621   \n",
       "std        18.447931       0.458515    8291.221345    9475.357325   \n",
       "min        -2.060000       2.472000       0.270000    -265.760000   \n",
       "25%        46.680000       2.933000    2240.270000      41.600000   \n",
       "50%        62.090000       3.452000    5347.450000     192.000000   \n",
       "75%        74.280000       3.738000    9210.900000    1926.940000   \n",
       "max       100.140000       4.468000   88646.760000  104519.540000   \n",
       "\n",
       "           MarkDown3      MarkDown4      MarkDown5            CPI  \\\n",
       "count  137091.000000  134967.000000  151432.000000  421570.000000   \n",
       "mean     1439.421384    3383.168256    4628.975079     171.201947   \n",
       "std      9623.078290    6292.384031    5962.887455      39.159276   \n",
       "min       -29.100000       0.220000     135.160000     126.064000   \n",
       "25%         5.080000     504.220000    1878.440000     132.022667   \n",
       "50%        24.600000    1481.310000    3359.450000     182.318780   \n",
       "75%       103.990000    3595.040000    5563.800000     212.416993   \n",
       "max    141630.610000   67474.850000  108519.280000     227.232807   \n",
       "\n",
       "        Unemployment           Year          Month            Day  WeekOfYear  \n",
       "count  421570.000000  421570.000000  421570.000000  421570.000000    421570.0  \n",
       "mean        7.960289    2010.968591       6.449510      15.673131   25.826762  \n",
       "std         1.863296       0.796876       3.243217       8.753549   14.151887  \n",
       "min         3.879000    2010.000000       1.000000       1.000000         1.0  \n",
       "25%         6.891000    2010.000000       4.000000       8.000000        14.0  \n",
       "50%         7.866000    2011.000000       6.000000      16.000000        26.0  \n",
       "75%         8.572000    2012.000000       9.000000      23.000000        38.0  \n",
       "max        14.313000    2012.000000      12.000000      31.000000        52.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = merged_test_df.columns\n",
    "target_col = 'Weekly_Sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(merged_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = train_df[input_cols]\n",
    "val_inputs = val_df[input_cols]\n",
    "test_inputs = merged_test_df[input_cols]\n",
    "\n",
    "train_targets = train_df[target_col]\n",
    "val_targets = val_df[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Store', 'Dept', 'Date', 'IsHoliday', 'Type', 'Size', 'Temperature',\n",
       "       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n",
       "       'MarkDown5', 'CPI', 'Unemployment', 'Year', 'Month', 'Day',\n",
       "       'WeekOfYear'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Size', 'Fuel_Price', 'CPI', 'Unemployment']\n",
    "categorical_cols = ['Store', 'Dept','IsHoliday','Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Size', 'Fuel_Price', 'CPI', 'Unemployment']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Store', 'Dept', 'IsHoliday', 'Type']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27, -265.76, -29.1, 0.22, 135.16)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_MarkDown1 = train_inputs.MarkDown1.min()\n",
    "min_MarkDown2 = train_inputs.MarkDown2.min()\n",
    "min_MarkDown3 = train_inputs.MarkDown3.min()\n",
    "min_MarkDown4 = train_inputs.MarkDown4.min()\n",
    "min_MarkDown5 = train_inputs.MarkDown5.min()\n",
    "min_MarkDown1, min_MarkDown2, min_MarkDown3, min_MarkDown4, min_MarkDown5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "imputer = SimpleImputer(fill_value=0)\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False, sparse_output=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse=False, sparse_output=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore', sparse=False, sparse_output=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit(merged_df[numeric_cols])\n",
    "imputer.fit(merged_df[numeric_cols])\n",
    "encoder.fit(merged_df[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1918363863.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1918363863.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1918363863.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1918363863.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n"
     ]
    }
   ],
   "source": [
    "train_inputs[numeric_cols] = scaler.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = scaler.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = scaler.transform(test_inputs[numeric_cols])\n",
    "\n",
    "train_inputs[numeric_cols] = imputer.transform(train_inputs[numeric_cols])\n",
    "val_inputs[numeric_cols] = imputer.transform(val_inputs[numeric_cols])\n",
    "test_inputs[numeric_cols] = imputer.transform(test_inputs[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_cols = list(encoder.get_feature_names_out(categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n",
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1584174743.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])\n"
     ]
    }
   ],
   "source": [
    "train_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\n",
    "val_inputs[encoded_cols] = encoder.transform(val_inputs[categorical_cols])\n",
    "test_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_inputs[numeric_cols+encoded_cols].copy()\n",
    "X_val = val_inputs[numeric_cols+encoded_cols].copy()\n",
    "X_test = test_inputs[numeric_cols+encoded_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "val_preds = model.predict(X_val)\n",
    "test_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13262.907113379444, 13442.817345612551)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error = mean_squared_error(train_preds, train_targets)\n",
    "val_error = mean_squared_error(val_preds, val_targets)\n",
    "np.sqrt(train_error), np.sqrt(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "rnd_reg = RandomForestRegressor(random_state=42, max_depth=60)\n",
    "svm_reg = SVR(kernel='poly', gamma='auto', coef0=1, shrinking=False)\n",
    "xgb_reg = XGBRegressor()\n",
    "\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[('rf', rnd_reg)]\n",
    ")\n",
    "\n",
    "# ('lr', lin_reg), ('rf', rnd_reg), ('svc', svm_reg), ('xgb', xgb_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_errors(train_inputs, train_targets, val_inputs, val_targets, model= voting_reg):\n",
    "    model.fit(train_inputs, train_targets)\n",
    "    train_preds = model.predict(train_inputs)\n",
    "    val_preds = model.predict(val_inputs)\n",
    "    train_error = mean_squared_error(train_preds, train_targets)\n",
    "    val_error = mean_squared_error(val_preds, val_targets)\n",
    "    return np.sqrt(train_error), np.sqrt(val_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14515761.742407847, 39177142.206385225)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error, val_error = predict_errors(X_train, train_targets, X_val, val_targets, voting_reg)\n",
    "train_error, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_plot(start_depth= 40, end_depth= 60):\n",
    "    train_rmse = []\n",
    "    val_rmse = []\n",
    "    index = []\n",
    "    for i in range(start_depth, end_depth+1):\n",
    "        model = RandomForestRegressor(random_state=42, max_depth=i)\n",
    "        train_error, val_error = predict_errors(X_train, train_targets, X_val, val_targets, model)\n",
    "        train_rmse.append(train_error)\n",
    "        val_rmse.append(val_error)\n",
    "        index.append(i)    \n",
    "    plt.plot(index, train_rmse, 'r-o')\n",
    "    plt.plot(index, val_rmse, 'b--x')\n",
    "    plt.legend('Train RMSE', 'Val RMSE')\n",
    "    plt.xlabel(\"Max-Depth\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return train_rmse, val_rmse, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1806603440.py:13: UserWarning: Legend does not support handles for str instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#controlling-the-legend-entries\n",
      "  plt.legend('Train RMSE', 'Val RMSE')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_rmse, val_rmse, index = predict_plot(45, 55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Local\\Temp\\ipykernel_30404\\1806603440.py:13: UserWarning: Legend does not support handles for str instances.\n",
      "A proxy artist may be used instead.\n",
      "See: https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html#controlling-the-legend-entries\n",
      "  plt.legend('Train RMSE', 'Val RMSE')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_rmse, val_rmse, index = predict_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(index, train_rmse, 'r-o')\n",
    "plt.plot(index, val_rmse, 'b--x')\n",
    "plt.legend('Train RMSE', 'Val RMSE')\n",
    "plt.xlabel(\"Max-Depth\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(random_state=42, max_depth=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2276.9463073841766, 5645.863719659226)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_error, val_error = predict_errors(X_train, train_targets, X_val, val_targets, model)\n",
    "train_error, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict(X_train)\n",
    "val_preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "continuous is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_score \u001b[39m=\u001b[39m accuracy_score(train_preds, train_targets)\n\u001b[0;32m      2\u001b[0m val_score \u001b[39m=\u001b[39m accuracy_score(val_preds, val_targets)\n\u001b[0;32m      3\u001b[0m train_score, val_score\n",
      "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:192\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    187\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    188\u001b[0m     parameter_constraints, params, caller_name\u001b[39m=\u001b[39mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\n\u001b[0;32m    189\u001b[0m )\n\u001b[0;32m    191\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    194\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    196\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    197\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m    199\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    200\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    201\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[0;32m    202\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:221\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[0;32m    157\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    222\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    223\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rohan\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:106\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m# No metrics support \"multiclass-multioutput\" format\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultilabel-indicator\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    109\u001b[0m     y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[1;31mValueError\u001b[0m: continuous is not supported"
     ]
    }
   ],
   "source": [
    "train_score = accuracy_score(train_preds, train_targets)\n",
    "val_score = accuracy_score(val_preds, val_targets)\n",
    "train_score, val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21249.6376    , 19133.0293    , 18762.8641    , ...,\n",
       "        2006.26756698,  2006.26756698,  2006.26756698])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "submission_preds = test_preds\n",
    "submission_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1_2012-11-02</td>\n",
       "      <td>21249.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_1_2012-11-09</td>\n",
       "      <td>19133.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1_2012-11-16</td>\n",
       "      <td>18762.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1_2012-11-23</td>\n",
       "      <td>20868.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_1_2012-11-30</td>\n",
       "      <td>19159.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115059</th>\n",
       "      <td>45_98_2013-06-28</td>\n",
       "      <td>2006.267567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115060</th>\n",
       "      <td>45_98_2013-07-05</td>\n",
       "      <td>2006.267567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115061</th>\n",
       "      <td>45_98_2013-07-12</td>\n",
       "      <td>2006.267567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115062</th>\n",
       "      <td>45_98_2013-07-19</td>\n",
       "      <td>2006.267567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115063</th>\n",
       "      <td>45_98_2013-07-26</td>\n",
       "      <td>2006.267567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115064 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Id  Weekly_Sales\n",
       "0         1_1_2012-11-02  21249.637600\n",
       "1         1_1_2012-11-09  19133.029300\n",
       "2         1_1_2012-11-16  18762.864100\n",
       "3         1_1_2012-11-23  20868.035600\n",
       "4         1_1_2012-11-30  19159.704800\n",
       "...                  ...           ...\n",
       "115059  45_98_2013-06-28   2006.267567\n",
       "115060  45_98_2013-07-05   2006.267567\n",
       "115061  45_98_2013-07-12   2006.267567\n",
       "115062  45_98_2013-07-19   2006.267567\n",
       "115063  45_98_2013-07-26   2006.267567\n",
       "\n",
       "[115064 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df['Weekly_Sales'] = submission_preds\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='./data/walmart-recruiting-store-sales-forecasting/submission.csv' target='_blank'>./data/walmart-recruiting-store-sales-forecasting/submission.csv</a><br>"
      ],
      "text/plain": [
       "d:\\Documents\\AI-ML-DL-Data\\Kaggle\\data\\walmart-recruiting-store-sales-forecasting\\submission.csv"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df.to_csv(DATA_DIR +'/submission.csv', index=None)\n",
    "from IPython.display import FileLink\n",
    "FileLink(DATA_DIR +'/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
